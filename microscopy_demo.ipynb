{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import tqdm\n",
    "import argparse\n",
    "import logging\n",
    "from PIL import Image \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import data, utils, models"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# definition for loading model from a pretrained network file\n",
    "\n",
    "def load_model(PATH, parallel=False, pretrained=True, old=True, load_opt=True):\n",
    "    state_dict = torch.load(PATH, map_location=\"cpu\")\n",
    "    args = argparse.Namespace(**{**vars(state_dict[\"args\"])})\n",
    "    if old:\n",
    "        vars(args)['blind_noise'] = False\n",
    "    \n",
    "    model = models.build_model(args)\n",
    "    # model.load_state_dict(state_dict[\"model\"][0])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    if load_opt:\n",
    "        for o, state in zip([optimizer], state_dict[\"optimizer\"]):\n",
    "            o.load_state_dict(state)\n",
    "    \n",
    "    if pretrained:\n",
    "        state_dict = torch.load(PATH)[\"model\"][0]\n",
    "        own_state = model.state_dict()\n",
    "        # print(own_state)\n",
    "\n",
    "        for name, param in state_dict.items():\n",
    "            if parallel:\n",
    "                name = name[7:]\n",
    "            if name not in own_state:\n",
    "                print(\"here\", name)\n",
    "                continue\n",
    "            if isinstance(param, nn.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            own_state[name].copy_(param)\n",
    "        \n",
    "    return model, optimizer, args"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE NUMBER\n",
    "example = 1 # Set 1 for another example"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# necessary variable deifnitons\n",
    "\n",
    "parallel = True\n",
    "Fast = False\n",
    "pretrained = True\n",
    "old = True\n",
    "load_opt = False\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "transform = transforms.Compose([transforms.ToPILImage()])\n",
    "to_gray = transforms.Compose([transforms.ToPILImage(), transforms.Grayscale(num_output_channels=1)])\n",
    "center = transforms.Compose([transforms.CenterCrop(40)])\n",
    "\n",
    "dataset = \"CTC\"\n",
    "video = f\"fluoro_{example+1}\"\n",
    "patch_size = 256\n",
    "stride = 128\n",
    "is_image = False\n",
    "n_frames = 5\n",
    "cpf = 1\n",
    "mid = n_frames // 2\n",
    "is_real = True\n",
    "\n",
    "aug = 3\n",
    "\n",
    "dist = 'G'\n",
    "mode = 'S'\n",
    "noise_std = 30\n",
    "min_noise = 0\n",
    "max_noise = 100\n",
    "\n",
    "batch_size = 1\n",
    "lr = 1e-4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load model\n",
    "\n",
    "PATH = \"pretrained/fluoro_micro.pt\"\n",
    "\n",
    "model, optimizer, args = load_model(PATH, parallel=parallel, pretrained=pretrained, old=old, load_opt=load_opt)\n",
    "model.to(device)\n",
    "print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data loader\n",
    "\n",
    "PATH = \"datasets/CTC\"\n",
    "train_loader, test_loader = data.build_dataset(\"CTC\", PATH, batch_size=1, image_size=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# denoise\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if example == 0:\n",
    "    num = 18; x = 575; y = 475; w = 200; h = 200\n",
    "elif example == 1:\n",
    "    num = 100; x = 442; y = 886; w = 128; h = 128\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = test_loader.dataset[num].unsqueeze(0)[:,:,y:y+h, x:x+w].to(device)\n",
    "    outputs, _ = model(inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
    "\n",
    "if is_image:\n",
    "    ax1.imshow(inputs[0,0,:,:].cpu().detach().numpy(), cmap=\"gray\", vmin=0, vmax=1) \n",
    "else:\n",
    "    ax1.imshow(inputs[0,2,:,:].cpu().detach().numpy(), cmap=\"gray\", vmin=0, vmax=1) \n",
    "ax1.set_title(\"Noisy input\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "ax2.imshow(outputs[0,0,:,:].cpu().detach().numpy(), cmap=\"gray\", vmin=0, vmax=1) \n",
    "ax2.set_title(\"Denoised frame\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# denoise entire video\n",
    "\n",
    "model.eval()\n",
    "\n",
    "frames = []\n",
    "os.makedirs(os.path.join(\"examples\", video), exist_ok=True)\n",
    "save_dir = os.path.join(\"examples\", video)\n",
    "C, H, W = test_loader.dataset[0].shape\n",
    "\n",
    "if example == 0:\n",
    "    x = 575; y = 475; w = 200; h = 200\n",
    "elif example == 1:\n",
    "    x = 450; y = 780; w = 200; h = 200\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, noisy_inputs in enumerate(test_loader):\n",
    "        if example == 0:\n",
    "            if i >= 48:\n",
    "                break\n",
    "        elif example == 1:\n",
    "            if i < 96:\n",
    "                continue\n",
    "            if i >= 188:\n",
    "                break\n",
    "        noisy_inputs = noisy_inputs[:,:,y:y+h,x:x+w].to(device)\n",
    "        \n",
    "        N, C, H, W = noisy_inputs.shape\n",
    "        noisy_frame = noisy_inputs[:, 2:3, :, :]\n",
    "        \n",
    "        output, _ = model(noisy_inputs)\n",
    "        \n",
    "        img = np.array((output[0]/max(1,output[0].max())).cpu().detach().numpy().reshape(w,h)*255, dtype=np.uint8)\n",
    "        noisy_img = np.array((noisy_frame[0]/max(1,noisy_frame[0].max())).cpu().detach().numpy().reshape(w,h)*255, dtype=np.uint8)\n",
    "        \n",
    "        frame = cv2.cvtColor(np.append(noisy_img, img, axis=1), cv2.COLOR_GRAY2BGR)\n",
    "        cv2.line(frame, (W,0), (W,H), (0,0,0), 1)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(save_dir, \"%05d.jpg\" % (i)), frame)\n",
    "        \n",
    "        frames.append(frame)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combining frames into mp4 video\n",
    "\n",
    "fps = 24\n",
    "\n",
    "video_path = os.path.join(save_dir, f\"{video}.mp4\")\n",
    "height, width, layers = frames[0].shape\n",
    "out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'MJPG'), fps, (width, height))\n",
    "for i in range(len(frames)):\n",
    "    out.write(frames[i])\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# converting mp4 to a gif video (optional)\n",
    "\n",
    "from moviepy.editor import *\n",
    "\n",
    "video_path = os.path.join(save_dir, f\"{video}.mp4\")\n",
    "gif_path = os.path.join(save_dir, f\"{video}.gif\")\n",
    "\n",
    "clip = (VideoFileClip(video_path).resize(0.5))\n",
    "clip.write_gif(gif_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}